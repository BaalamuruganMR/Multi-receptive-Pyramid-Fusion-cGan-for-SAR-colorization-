{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kSXBzHuY-bzk",
        "outputId": "55c826d7-609c-47d8-b9d0-49a587d71be9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "# # mount your Google Drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lsKS3Le_-tJ0"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.chdir(\"/content/drive/My Drive/Colab Notebooks/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bGFXRAkc-vBD",
        "outputId": "2580fa9e-3db0-4d95-fc7d-5d9cd16f4ac5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  python3-gdal python3-numpy\n",
            "Suggested packages:\n",
            "  libgdal-grass python-numpy-doc python3-pytest\n",
            "The following NEW packages will be installed:\n",
            "  gdal-bin python3-gdal python3-numpy\n",
            "0 upgraded, 3 newly installed, 0 to remove and 49 not upgraded.\n",
            "Need to get 5,055 kB of archives.\n",
            "After this operation, 25.1 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 python3-numpy amd64 1:1.21.5-1ubuntu22.04.1 [3,467 kB]\n",
            "Get:2 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy/main amd64 python3-gdal amd64 3.6.4+dfsg-1~jammy0 [1,027 kB]\n",
            "Get:3 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy/main amd64 gdal-bin amd64 3.6.4+dfsg-1~jammy0 [561 kB]\n",
            "Fetched 5,055 kB in 2s (2,720 kB/s)\n",
            "Selecting previously unselected package python3-numpy.\n",
            "(Reading database ... 123630 files and directories currently installed.)\n",
            "Preparing to unpack .../python3-numpy_1%3a1.21.5-1ubuntu22.04.1_amd64.deb ...\n",
            "Unpacking python3-numpy (1:1.21.5-1ubuntu22.04.1) ...\n",
            "Selecting previously unselected package python3-gdal.\n",
            "Preparing to unpack .../python3-gdal_3.6.4+dfsg-1~jammy0_amd64.deb ...\n",
            "Unpacking python3-gdal (3.6.4+dfsg-1~jammy0) ...\n",
            "Selecting previously unselected package gdal-bin.\n",
            "Preparing to unpack .../gdal-bin_3.6.4+dfsg-1~jammy0_amd64.deb ...\n",
            "Unpacking gdal-bin (3.6.4+dfsg-1~jammy0) ...\n",
            "Setting up python3-numpy (1:1.21.5-1ubuntu22.04.1) ...\n",
            "Setting up python3-gdal (3.6.4+dfsg-1~jammy0) ...\n",
            "Setting up gdal-bin (3.6.4+dfsg-1~jammy0) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n"
          ]
        }
      ],
      "source": [
        "!apt install gdal-bin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uqmjg-N0-vej"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import sys\n",
        "import time\n",
        "#import gdal\n",
        "from PIL import Image\n",
        "import skimage\n",
        "import tensorflow as tf\n",
        "import keras.backend as K\n",
        "\n",
        "from osgeo import gdal\n",
        "from skimage.transform import resize\n",
        "from keras import Model\n",
        "from keras.layers import Input, Conv2D, Conv2DTranspose, Lambda, Dropout, BatchNormalization, Add, Concatenate\n",
        "from keras.optimizers import Adam\n",
        "from keras.layers import LeakyReLU\n",
        "from keras.layers import  Activation, Reshape, Dense, Flatten, ZeroPadding2D\n",
        "from keras.models import save_model, load_model\n",
        "from keras.initializers import RandomNormal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JP28zHCu-3C7"
      },
      "outputs": [],
      "source": [
        "def read_tiff(tiff_file):\n",
        "    data = gdal.Open(tiff_file).ReadAsArray()\n",
        "    return data\n",
        "\n",
        "def load_sentinel_data(path):\n",
        "    img_paths = sorted(glob.glob(path + '*.tif'))\n",
        "    image = [np.expand_dims(read_tiff(img).astype('float32'), -1) for img in img_paths]\n",
        "    image = np.concatenate(image, axis=-1)\n",
        "    print(\"Image shape: \", image.shape, \" Min value: \", image.min(), \" Max value: \", image.max())\n",
        "    return image\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "mHAVcwAm-3rz",
        "outputId": "85a4f983-3faa-4a1f-d1f3-6dd9932b5ff1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Image shape:  (13, 256, 256, 400)  Min value:  -26.463856  Max value:  21478.363\n",
            "Image shape:  (2, 256, 256, 400)  Min value:  -58.021236  Max value:  30.682518\n"
          ]
        }
      ],
      "source": [
        "S2_img = load_sentinel_data('/content/drive/MyDrive/train/s2/')\n",
        "S1_img = load_sentinel_data('/content/drive/MyDrive/train/s1/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X8e6ow6Y-95b"
      },
      "outputs": [],
      "source": [
        "def create_idx_image(rows, cols):\n",
        "    im_idx = np.arange(rows * cols).reshape(rows, cols)\n",
        "    return im_idx\n",
        "\n",
        "def extract_patches(im_idx, patch_size, overlap):\n",
        "    '''overlap range: 0 - 1 '''\n",
        "    row_steps, cols_steps = int((1-overlap) * patch_size[0]), int((1-overlap) * patch_size[1])\n",
        "    patches = skimage.util.view_as_windows(im_idx, patch_size, step=(row_steps, cols_steps))\n",
        "    return patches\n",
        "\n",
        "def define_trn_val_tst_mask(tiles_grid_idx, grid_size=(10,10), val_tiles=None, tst_tiles=None, plot=True):\n",
        "    num_tiles_rows = grid_size[0]\n",
        "    num_tiles_cols = grid_size[1]\n",
        "\n",
        "    tiles_idx = np.arange(len(tiles_grid_idx))\n",
        "    if val_tiles and tst_tiles:\n",
        "        val_tiles_idx = val_tiles\n",
        "        tst_tiles_idx = tst_tiles\n",
        "        trn_tiles_idx = set(tiles_idx) - set(val_tiles_idx) - set(tst_tiles_idx)\n",
        "    else:\n",
        "        tiles_idx = np.random.permutation(tiles_idx)\n",
        "        trn_tiles_idx = tiles_idx[:int(0.9*len(tiles_idx))]\n",
        "        val_tiles_idx = tiles_idx[int(0.9*len(tiles_idx)):int(0.95*len(tiles_idx))]\n",
        "        tst_tiles_idx = tiles_idx[int(0.95*len(tiles_idx)):]\n",
        "\n",
        "    print(val_tiles_idx, tst_tiles_idx)\n",
        "\n",
        "    tiles_numbers = np.zeros_like(tiles_grid_idx, dtype='uint8')\n",
        "    for i in range(len(tiles_grid_idx)):\n",
        "        tiles_numbers[i] = i\n",
        "\n",
        "    mask = np.zeros_like(tiles_grid_idx, dtype='uint8')\n",
        "    for idx in val_tiles_idx:\n",
        "        mask[tiles_numbers==idx] = 1\n",
        "    for idx in tst_tiles_idx:\n",
        "        mask[tiles_numbers==idx] = 2\n",
        "\n",
        "    mask = mask_tiles(mask.reshape(num_tiles_rows, num_tiles_cols, rows//num_tiles_rows, cols//num_tiles_cols))\n",
        "    if plot:\n",
        "        plt.figure(figsize=(5,5))\n",
        "        plt.imshow(mask, cmap='PuBuGn')\n",
        "        plt.axis('off')\n",
        "        plt.show()\n",
        "        plt.close()\n",
        "    return mask\n",
        "\n",
        "def mask_tiles(patches):\n",
        "    num_blocks_r, num_blocks_c, rows_block, cols_block = patches.shape\n",
        "    img = np.zeros((num_blocks_r*rows_block, num_blocks_c*cols_block), dtype=patches.dtype)\n",
        "    for i in range(num_blocks_r):\n",
        "        for j in range(num_blocks_c):\n",
        "            img[rows_block*i:(rows_block*i+rows_block), cols_block*j:(cols_block*j+cols_block)] = patches[i,j]\n",
        "    return img\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_sCzEf_i_Eck"
      },
      "outputs": [],
      "source": [
        "# define the image of indexes\n",
        "rows, cols = S2_img.shape[:2]\n",
        "im_idx = create_idx_image(rows, cols)\n",
        "\n",
        "# Compute grids of tiles\n",
        "grid_size = (3, 3)\n",
        "tiles_grid_idx = extract_patches(im_idx, patch_size=(rows//grid_size[0], cols//grid_size[1]), overlap=0).reshape(-1, rows//grid_size[0], cols//grid_size[0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rFnyjqc1_Hn7",
        "outputId": "ecf701b8-eac7-4995-de64-3bceea76d124"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1, 85)"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tiles_grid_idx[2].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 58
        },
        "id": "mbXS7-59_MVz",
        "outputId": "2e1c0abd-5a33-47e6-e184-8804c23912c6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2, 4, 6] [9, 10, 11]\n"
          ]
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZcAAAAYCAYAAAA297C0AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAAABHUlEQVR4nO3coUpDYRjH4W9ikDWx2sYYrBiWvAAFQbRavANvwGIxWHYjNsEgos0kWASLzCBYRdswiMdL2Ae+4/Xo8+Qv/NuPFw6n0zRNUwAg0EL2AAD+HnEBIJy4ABBOXAAIJy4AhBMXAMKJCwDhxAWAcIvVLz8+5zgDaIPzu5eye7yXPYNkX5e3M9/Ux4V/7+HptUzeptkzSHT/Pi3DzaPsGbSAuFBt5+KmPF+Ps2eQqL9xWB4PtrNn0AKd2n+LDcZnc57Cb7c/6Je15W72DBL1lrtl2FvJnkG2pdl3SfXlMrk6+dEW2m+0flq2RqvZM4AWqL5cAKCWT5EBCCcuAIQTFwDCiQsA4cQFgHDiAkA4cQEgnLgAEE5cAAj3DQpxKleQjVlqAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 500x500 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Computing and plotting trn, val and tst mask.\n",
        "# val_tiles=[21, 47, 82]\n",
        "# tst_tiles=[13, 54, 87]\n",
        "val_tiles=[0, 2, 4]\n",
        "tst_tiles=[6, 7, 8]\n",
        "mask_trn_val_tst = define_trn_val_tst_mask(tiles_grid_idx, grid_size, val_tiles, tst_tiles)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N2zWT33mRsmL",
        "outputId": "a105beab-57fe-4c68-9e63-565a2453bf04"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(3, 256)\n",
            "(3, 255)\n"
          ]
        }
      ],
      "source": [
        "print(im_idx.shape)\n",
        "print(mask_trn_val_tst.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iMRO650WSNIS"
      },
      "outputs": [],
      "source": [
        "from skimage.transform import resize\n",
        "im_idx_resized = resize(im_idx, (256, 256))\n",
        "mask_trn_val_tst_resized = resize(mask_trn_val_tst, (256, 256))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W7PbXwudSklM",
        "outputId": "de7043d2-64c8-43ad-9659-d4edc1387af8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(3, 256)\n",
            "(3, 255)\n"
          ]
        }
      ],
      "source": [
        "print(im_idx.shape)\n",
        "print(mask_trn_val_tst.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "MQLqLGW-_O4b",
        "outputId": "1f167867-c60b-45bd-ef2b-57039752e131"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR:root:Internal Python error in the inspect module.\n",
            "Below is the traceback from this internal error.\n",
            "\n",
            "ERROR:root:Internal Python error in the inspect module.\n",
            "Below is the traceback from this internal error.\n",
            "\n",
            "ERROR:root:Internal Python error in the inspect module.\n",
            "Below is the traceback from this internal error.\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-18-59e2781530c7>\", line 2, in <cell line: 2>\n",
            "    patches_idx = extract_patches(im_idx, patch_size=(256, 256), overlap=overlap).reshape(-1, 256, 256)\n",
            "  File \"<ipython-input-11-b0933f348e9f>\", line 8, in extract_patches\n",
            "    patches = skimage.util.view_as_windows(im_idx, patch_size, step=(row_steps, cols_steps))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/skimage/util/shape.py\", line 228, in view_as_windows\n",
            "    raise ValueError(\"`window_shape` is too large\")\n",
            "ValueError: `window_shape` is too large\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n",
            "    stb = value._render_traceback_()\n",
            "AttributeError: 'ValueError' object has no attribute '_render_traceback_'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
            "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 248, in wrapped\n",
            "    return f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n",
            "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 1662, in getinnerframes\n",
            "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 1620, in getframeinfo\n",
            "    filename = getsourcefile(frame) or getfile(frame)\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 829, in getsourcefile\n",
            "    module = getmodule(object, filename)\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 861, in getmodule\n",
            "    file = getabsfile(object, _filename)\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 845, in getabsfile\n",
            "    return os.path.normcase(os.path.abspath(_filename))\n",
            "  File \"/usr/lib/python3.10/posixpath.py\", line 384, in abspath\n",
            "    cwd = os.getcwd()\n",
            "OSError: [Errno 107] Transport endpoint is not connected\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-18-59e2781530c7>\", line 2, in <cell line: 2>\n",
            "    patches_idx = extract_patches(im_idx, patch_size=(256, 256), overlap=overlap).reshape(-1, 256, 256)\n",
            "  File \"<ipython-input-11-b0933f348e9f>\", line 8, in extract_patches\n",
            "    patches = skimage.util.view_as_windows(im_idx, patch_size, step=(row_steps, cols_steps))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/skimage/util/shape.py\", line 228, in view_as_windows\n",
            "    raise ValueError(\"`window_shape` is too large\")\n",
            "ValueError: `window_shape` is too large\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n",
            "    stb = value._render_traceback_()\n",
            "AttributeError: 'ValueError' object has no attribute '_render_traceback_'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3575, in run_code\n",
            "    self.showtraceback(running_compiled_code=True)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2101, in showtraceback\n",
            "    stb = self.InteractiveTB.structured_traceback(etype,\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1367, in structured_traceback\n",
            "    return FormattedTB.structured_traceback(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1267, in structured_traceback\n",
            "    return VerboseTB.structured_traceback(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1124, in structured_traceback\n",
            "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1082, in format_exception_as_a_whole\n",
            "    last_unique, recursion_repeat = find_recursion(orig_etype, evalue, records)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 382, in find_recursion\n",
            "    return len(records), 0\n",
            "TypeError: object of type 'NoneType' has no len()\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n",
            "    stb = value._render_traceback_()\n",
            "AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
            "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 248, in wrapped\n",
            "    return f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n",
            "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 1662, in getinnerframes\n",
            "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 1620, in getframeinfo\n",
            "    filename = getsourcefile(frame) or getfile(frame)\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 829, in getsourcefile\n",
            "    module = getmodule(object, filename)\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 861, in getmodule\n",
            "    file = getabsfile(object, _filename)\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 845, in getabsfile\n",
            "    return os.path.normcase(os.path.abspath(_filename))\n",
            "  File \"/usr/lib/python3.10/posixpath.py\", line 384, in abspath\n",
            "    cwd = os.getcwd()\n",
            "OSError: [Errno 107] Transport endpoint is not connected\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-18-59e2781530c7>\", line 2, in <cell line: 2>\n",
            "    patches_idx = extract_patches(im_idx, patch_size=(256, 256), overlap=overlap).reshape(-1, 256, 256)\n",
            "  File \"<ipython-input-11-b0933f348e9f>\", line 8, in extract_patches\n",
            "    patches = skimage.util.view_as_windows(im_idx, patch_size, step=(row_steps, cols_steps))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/skimage/util/shape.py\", line 228, in view_as_windows\n",
            "    raise ValueError(\"`window_shape` is too large\")\n",
            "ValueError: `window_shape` is too large\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n",
            "    stb = value._render_traceback_()\n",
            "AttributeError: 'ValueError' object has no attribute '_render_traceback_'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3575, in run_code\n",
            "    self.showtraceback(running_compiled_code=True)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2101, in showtraceback\n",
            "    stb = self.InteractiveTB.structured_traceback(etype,\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1367, in structured_traceback\n",
            "    return FormattedTB.structured_traceback(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1267, in structured_traceback\n",
            "    return VerboseTB.structured_traceback(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1124, in structured_traceback\n",
            "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1082, in format_exception_as_a_whole\n",
            "    last_unique, recursion_repeat = find_recursion(orig_etype, evalue, records)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 382, in find_recursion\n",
            "    return len(records), 0\n",
            "TypeError: object of type 'NoneType' has no len()\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n",
            "    stb = value._render_traceback_()\n",
            "AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3492, in run_ast_nodes\n",
            "    self.showtraceback()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2101, in showtraceback\n",
            "    stb = self.InteractiveTB.structured_traceback(etype,\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1367, in structured_traceback\n",
            "    return FormattedTB.structured_traceback(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1267, in structured_traceback\n",
            "    return VerboseTB.structured_traceback(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1142, in structured_traceback\n",
            "    formatted_exceptions += self.format_exception_as_a_whole(etype, evalue, etb, lines_of_context,\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1082, in format_exception_as_a_whole\n",
            "    last_unique, recursion_repeat = find_recursion(orig_etype, evalue, records)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 382, in find_recursion\n",
            "    return len(records), 0\n",
            "TypeError: object of type 'NoneType' has no len()\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n",
            "    stb = value._render_traceback_()\n",
            "AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
            "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 248, in wrapped\n",
            "    return f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n",
            "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 1662, in getinnerframes\n",
            "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 1620, in getframeinfo\n",
            "    filename = getsourcefile(frame) or getfile(frame)\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 829, in getsourcefile\n",
            "    module = getmodule(object, filename)\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 861, in getmodule\n",
            "    file = getabsfile(object, _filename)\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 845, in getabsfile\n",
            "    return os.path.normcase(os.path.abspath(_filename))\n",
            "  File \"/usr/lib/python3.10/posixpath.py\", line 384, in abspath\n",
            "    cwd = os.getcwd()\n",
            "OSError: [Errno 107] Transport endpoint is not connected\n"
          ]
        }
      ],
      "source": [
        "overlap=0.0\n",
        "patches_idx = extract_patches(im_idx, patch_size=(256, 256), overlap=overlap).reshape(-1, 256, 256)\n",
        "patches_mask = extract_patches(mask_trn_val_tst, patch_size=(256, 256), overlap=overlap).reshape(-1, 256, 256)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gD4uFnDi_TD6",
        "outputId": "75071442-4487-4197-9665-13dd4d6668d6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of training and validation patches:   0 0\n"
          ]
        }
      ],
      "source": [
        "# Selecting index trn val and test patches idx\n",
        "idx_trn = np.squeeze(np.where(patches_mask.sum(axis=(1, 2))==0))\n",
        "idx_val = np.squeeze(np.where(patches_mask.sum(axis=(1, 2))==256**2))\n",
        "# idx_tst = np.squeeze(np.where(patches_mask.sum(axis=(1, 2))==2*256**2))\n",
        "\n",
        "patches_idx_trn = patches_idx[idx_trn]\n",
        "patches_idx_val = patches_idx[idx_val]\n",
        "\n",
        "# print\n",
        "print('Number of training and validation patches:  ', len(idx_trn), len(idx_val))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9GDZwamY_Vs7"
      },
      "outputs": [],
      "source": [
        "def db2intensities(img):\n",
        "    img = 10**(img/10.0)\n",
        "    return img\n",
        "\n",
        "def filter_outliers(img, bins=1000000, bth=0.001, uth=0.999, mask=[0]):\n",
        "    img[np.isnan(img)]=0 # Filer nan values.\n",
        "    if len(mask)==1:\n",
        "        mask = np.zeros((img.shape[:2]), dtype='int64')\n",
        "    for band in range(img.shape[-1]):\n",
        "        hist = np.histogram(img[:mask.shape[0], :mask.shape[1]][mask!=2, band].ravel(),bins=bins) # select not testing pixels\n",
        "        cum_hist = np.cumsum(hist[0])/hist[0].sum()\n",
        "        max_value = np.ceil(100*hist[1][len(cum_hist[cum_hist<uth])])/100\n",
        "        min_value = np.ceil(100*hist[1][len(cum_hist[cum_hist<bth])])/100\n",
        "        img[:,:, band][img[:,:, band]>max_value] = max_value\n",
        "        img[:,:, band][img[:,:, band]<min_value] = min_value\n",
        "    return img\n",
        "\n",
        "def normalize(img):\n",
        "    '''image shape: [row, cols, channels]'''\n",
        "    img = 2*(img -img.min(axis=(0,1), keepdims=True))/(img.max(axis=(0,1), keepdims=True) - img.min(axis=(0,1), keepdims=True)) - 1\n",
        "    return img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I7Jgt3RjAQ05"
      },
      "outputs": [],
      "source": [
        "# to intensity\n",
        "S1_img = db2intensities(S1_img)\n",
        "S1_img.min(), S1_img.max()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4bTx-WD3Amn0"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10,5))\n",
        "plt.title(\"Histogram of Sentinel 2 image\")\n",
        "plt.hist(S2_img[:,:,0].ravel(), bins=100)\n",
        "plt.grid()\n",
        "plt.show()\n",
        "plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Stpjx2xAn5V"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10,10))\n",
        "plt.imshow(S2_img[:,:,:3]/S2_img[:,:,:3].max())\n",
        "plt.show()\n",
        "plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p-USF_ntAqRh"
      },
      "outputs": [],
      "source": [
        "# Preprocessing images, delecting outliers\n",
        "S1_img = filter_outliers(S1_img.copy())\n",
        "S2_img = filter_outliers(S2_img.copy(), bins=int(2**16/2), mask=mask_trn_val_tst)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3QhIeF6jAs2p"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10,5))\n",
        "plt.title(\"Histogram of Sentinel 2 image after preprocessing\")\n",
        "plt.hist(S2_img[:,:,0].ravel(), bins=100)\n",
        "plt.grid()\n",
        "plt.show()\n",
        "plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kxsFyoWcAukZ"
      },
      "outputs": [],
      "source": [
        "# Nomalizing images between [-1 1]\n",
        "S1_img = normalize(S1_img)\n",
        "S2_img = normalize(S2_img)\n",
        "S1_img.min(), S1_img.max(), S2_img.min(), S2_img.max()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ws-9-raAzJC"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10,10))\n",
        "plt.imshow((S2_img[:,:,[0,1,2]] + 1)/2)\n",
        "plt.show()\n",
        "plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wle33afVA07Z"
      },
      "outputs": [],
      "source": [
        "# Architecure Parameters\n",
        "c_dim_sar = S1_img.shape[-1]\n",
        "c_dim_opt = S2_img.shape[-1]\n",
        "n_rows = 256\n",
        "n_cols = 256"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q862rXiUA28K"
      },
      "outputs": [],
      "source": [
        "# processing_images\n",
        "def randomResizeCrop(img_s1, img_s2, load_size=286, fine_size=256, flip=True):\n",
        "\n",
        "    img_s1 = resize(img_s1, [load_size, load_size], 0)\n",
        "    img_s2 = resize(img_s2, [load_size, load_size], 0)\n",
        "\n",
        "    h1 = int(np.ceil(np.random.uniform(1e-2, load_size-fine_size)))\n",
        "    w1 = int(np.ceil(np.random.uniform(1e-2, load_size-fine_size)))\n",
        "    img_s1 = img_s1[h1:h1+fine_size, w1:w1+fine_size]\n",
        "    img_s2 = img_s2[h1:h1+fine_size, w1:w1+fine_size]\n",
        "\n",
        "    if flip and np.random.random() > 0.5:\n",
        "        img_s1 = np.fliplr(img_s1)\n",
        "        img_s2 = np.fliplr(img_s2)\n",
        "\n",
        "    return img_s1, img_s2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v4BoyR9lA5YZ"
      },
      "outputs": [],
      "source": [
        "# define an encoder block\n",
        "def encoder_block(input_data, n_filters, k_size=3, strides=2, activation='relu', padding='same', batchnorm=True, name='None'):\n",
        "    # weight initialization\n",
        "    init = RandomNormal(stddev=0.02)\n",
        "    x = Conv2D(n_filters, k_size, strides=strides, padding=padding, kernel_initializer=init, name=name+'_conv2D')(input_data)\n",
        "    if batchnorm:\n",
        "        x = BatchNormalization(momentum=0.8, name=name+'_bn')(x, training=True)\n",
        "    if activation is 'LReLU':\n",
        "        x = LeakyReLU(alpha=0.2, name=name+'_act_LReLU')(x)\n",
        "    else:\n",
        "        x = Activation('relu', name=name+'_act_relu')(x)\n",
        "    return x\n",
        "\n",
        "# define a decoder block\n",
        "def decoder_block(input_data, n_filters, k_size=3, strides=2, padding='same', name='None'):\n",
        "    # weight initialization\n",
        "    init = RandomNormal(stddev=0.02)\n",
        "    x = Conv2DTranspose(n_filters, k_size, strides=strides, padding=padding, kernel_initializer=init, name=name+'_deconv2D')(input_data)\n",
        "    x = BatchNormalization(momentum=0.8, name=name+'_bn')(x, training=True)\n",
        "    x = Activation('relu', name=name+'_act_relu')(x)\n",
        "    return x\n",
        "\n",
        "def residual_block(input_x, n_kernels, name='name'):\n",
        "    x = encoder_block(input_x, n_kernels, strides=1, name=name+'rba')\n",
        "    x = Dropout(0.5, name=name+'drop')(x, training=True)\n",
        "    x = encoder_block(x, n_kernels,  strides=1, activation='linear', name=name+'rbb')\n",
        "    x = Add(name=name+'concatenate')([x, input_x])\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7wwD2iCbA8u5"
      },
      "outputs": [],
      "source": [
        "def build_generator2D(model_shape, filters=64, last_act='tanh', n_residuals=9, summary=False, model_file=None, name='gan_g_'):\n",
        "\n",
        "    init = RandomNormal(stddev=0.02)\n",
        "    n_rows = model_shape[0]\n",
        "    n_cols = model_shape[1]\n",
        "    in_c_dims = model_shape[2]\n",
        "    out_c_dims = model_shape[3]\n",
        "\n",
        "    input_shape = (n_rows, n_cols, in_c_dims)\n",
        "    input_layer = Input(shape=input_shape, name=name+'_input')\n",
        "\n",
        "    x = input_layer\n",
        "    x = encoder_block(x, 1*filters, k_size=7, strides=1, batchnorm=False, name=name+'_e1')\n",
        "    x = encoder_block(x, 2*filters, name=name+'e2') # rows/2, cols/2\n",
        "    x = encoder_block(x, 4*filters, name=name+'e3') # rows/4, cols/4\n",
        "\n",
        "    for i in range(n_residuals):\n",
        "        x = residual_block(x, n_kernels=4*filters, name=name+str(i+1)+'_')  # rows/4, cols/4\n",
        "\n",
        "    x = decoder_block(x, 2*filters, name=name+'d1') # rows/2, cols/2\n",
        "    x = decoder_block(x, 1*filters, name=name+'d2') # rows, cols\n",
        "    x = Conv2D(out_c_dims, 7, padding='same',  kernel_initializer=init, name=name+'d_out')(x)   # rows, cols\n",
        "\n",
        "    output = Activation(last_act, name=name+last_act)(x)\n",
        "\n",
        "    model = Model(inputs=[input_layer], outputs=[output], name='Generator'+name[-3:])\n",
        "    if (summary):\n",
        "        model.summary()\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v_B4UISpA_FJ"
      },
      "outputs": [],
      "source": [
        "# Setting the model shape\n",
        "gen_shape = (n_rows, n_cols, c_dim_sar, c_dim_opt)\n",
        "generator = build_generator2D(gen_shape, filters=64, name='gen', summary=True)\n",
        "tf.keras.utils.plot_model(generator, show_shapes=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8LVMMLiYBBxJ"
      },
      "outputs": [],
      "source": [
        "########################  NETWORK ARCHITECTURES  ###############################\n",
        "def build_discriminator2D(input_shape, filters=64, name='d', summary=False):\n",
        "    \"\"\"\n",
        "    Create a Discriminator Model using hyperparameters values defined as follows\n",
        "    \"\"\"\n",
        "    # weight initialization\n",
        "    init = RandomNormal(stddev=0.02)\n",
        "\n",
        "    input_img  = Input(shape=input_shape, name=name+'_input')\n",
        "    d = input_img\n",
        "    d = encoder_block(d, 1*filters, k_size=4, activation='LReLU', batchnorm=False, name=name+'_1')\n",
        "    d = encoder_block(d, 2*filters, k_size=4, activation='LReLU', name=name+'_2')\n",
        "    d = encoder_block(d, 4*filters, k_size=4, activation='LReLU', name=name+'_3')\n",
        "\n",
        "    d = ZeroPadding2D()(d)\n",
        "    d = encoder_block(d, 8*filters, k_size=4, activation='LReLU', strides=1, padding='valid', name=name+'_4')\n",
        "    d = ZeroPadding2D()(d)\n",
        "    logits = Conv2D(1, (4,4), padding='valid', kernel_initializer=init, name=name+'_conv2D_5')(d)\n",
        "    out = Activation('sigmoid', name=name+'_act_sigmoid')(logits)\n",
        "\n",
        "    model = Model(inputs=[input_img], outputs=[out, logits], name=name)\n",
        "    if (summary):\n",
        "        model.summary()\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FDnT6fbiBHhx"
      },
      "outputs": [],
      "source": [
        "dis_shape = (n_rows, n_cols, c_dim_sar + c_dim_opt)\n",
        "discriminator = build_discriminator2D(dis_shape, filters=64, name='dis', summary=True)\n",
        "# tf.keras.utils.plot_model(discriminator, show_shapes=True, dpi=64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b1faKnOmBLRR"
      },
      "outputs": [],
      "source": [
        "def cross_entropy_loss(labels, logits):\n",
        "    loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=labels))\n",
        "    return loss\n",
        "\n",
        "def lsgan_loss(labels, logits):\n",
        "        loss = tf.reduce_mean(tf.squared_difference(logits, labels))\n",
        "        return loss\n",
        "\n",
        "def l1_loss(a, b):\n",
        "    loss = tf.reduce_mean(tf.abs(a - b))\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7T-Ar3qOBOPo"
      },
      "outputs": [],
      "source": [
        "K.clear_session() # cleaning previous sessions\n",
        "\n",
        "batch_size = 1\n",
        "LAMBDA = 100\n",
        "\n",
        "# BUILDING NETWORKS GRAPHS\n",
        "Generator = build_generator2D(gen_shape, filters=64, name='gen', summary=False)\n",
        "Discriminator = build_discriminator2D(dis_shape, filters=64, name='dis', summary=False)\n",
        "\n",
        "# GRAPH INPUT DATA\n",
        "REAL_SAR = tf.placeholder(tf.float32,\n",
        "                        [batch_size, n_rows, n_cols, c_dim_sar],\n",
        "                        name='real_sar')\n",
        "\n",
        "REAL_OPT = tf.placeholder(tf.float32,\n",
        "                        [batch_size, n_rows, n_cols, c_dim_opt],\n",
        "                        name='real_opt')\n",
        "\n",
        "FAKE_OPT = Generator(REAL_SAR)\n",
        "\n",
        "REAL_PAIR = tf.concat([REAL_SAR, REAL_OPT], axis=-1)\n",
        "FAKE_PAIR = tf.concat([REAL_SAR, FAKE_OPT], axis=-1)\n",
        "\n",
        "# Discriminator\n",
        "D_real, D_real_logits = Discriminator(REAL_PAIR)\n",
        "D_fake, D_fake_logits = Discriminator(FAKE_PAIR)\n",
        "\n",
        "# Discriminator loss,\n",
        "d_loss_real = cross_entropy_loss(tf.ones_like(D_real), D_real_logits)\n",
        "d_loss_fake = cross_entropy_loss(tf.zeros_like(D_real), D_fake_logits)\n",
        "d_loss = (d_loss_real + d_loss_fake) / 2.0\n",
        "\n",
        "# Reconstruction loss\n",
        "reco_loss = LAMBDA * l1_loss(REAL_OPT, FAKE_OPT)\n",
        "\n",
        "# Generator loss,\n",
        "g_loss_ = cross_entropy_loss(tf.ones_like(D_fake), D_fake_logits)\n",
        "g_loss =  g_loss_ + reco_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r1OW_kuzBQlh"
      },
      "outputs": [],
      "source": [
        "# Collecting variables for training\n",
        "t_vars = tf.trainable_variables()\n",
        "\n",
        "d_vars = [var for var in t_vars if 'dis' in var.name] # Discriminator variables\n",
        "g_vars = [var for var in t_vars if 'gen' in var.name] # Generator variables\n",
        "\n",
        "# Optimizer parameters\n",
        "lr_d = 0.0002\n",
        "lr_g = 0.0002\n",
        "beta1 = 0.5\n",
        "\n",
        "# Assings variables and corresponding lossses to be minimized\n",
        "d_optim = tf.train.AdamOptimizer(lr_d, beta1=beta1).minimize(d_loss, var_list=d_vars)\n",
        "g_optim = tf.train.AdamOptimizer(lr_g, beta1=beta1).minimize(g_loss, var_list=g_vars)\n",
        "\n",
        "# Build the graph\n",
        "sess = tf.Session()\n",
        "\n",
        "# Initialize the all variables\n",
        "init_op = tf.global_variables_initializer()\n",
        "sess.run(init_op)\n",
        "\n",
        "# Add ops to save and restore all the variables.\n",
        "saver = tf.train.Saver()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "faSlaa7VBTdp"
      },
      "outputs": [],
      "source": [
        "epochs = 100\n",
        "patience = 15 # for early stoppig\n",
        "num_of_trn_batches = len(patches_idx_trn) // batch_size\n",
        "num_of_val_batches = len(patches_idx_val) // batch_size\n",
        "\n",
        "g_steps = 1\n",
        "\n",
        "saving_path = './'\n",
        "\n",
        "name = 'sar2opt'\n",
        "\n",
        "restore = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0GsnSnlBBWOo"
      },
      "outputs": [],
      "source": [
        "def get_next_batch_generator(patches_idx, s1data, s2data, batch_size=1, shuffle=True, flip=True, train=True):\n",
        "    num_of_batches = len(patches_idx) // batch_size\n",
        "    s1data = s1data.reshape(-1, s1data.shape[-1])\n",
        "    s2data = s2data.reshape(-1, s2data.shape[-1])\n",
        "    if shuffle:\n",
        "        np.random.shuffle(patches_idx)\n",
        "    while True:\n",
        "        for idx in range(num_of_batches):\n",
        "            patches_idx_batch = patches_idx[idx*batch_size:(idx+1)*batch_size]\n",
        "            batch_s1_patches, batch_s2_patches = [], []\n",
        "            for batch_idx in patches_idx_batch:\n",
        "                batch_s1, batch_s2 = s1data[batch_idx], s2data[batch_idx]\n",
        "                if train:\n",
        "                    # data augmentation\n",
        "                    batch_s1, batch_s2 = randomResizeCrop(batch_s1, batch_s2, flip=flip)\n",
        "                batch_s1_patches.append(batch_s1)\n",
        "                batch_s2_patches.append(batch_s2)\n",
        "\n",
        "            yield np.array(batch_s1_patches), np.array(batch_s2_patches)\n",
        "\n",
        "        if shuffle:\n",
        "            np.random.shuffle(patches_idx)\n",
        "        idx = 0\n",
        "\n",
        "def plot_images(sar, real_opt, fake_opt, figsize=(10, 5)):\n",
        "    fig = plt.figure(figsize=figsize)\n",
        "\n",
        "    ax1 = fig.add_subplot(131)\n",
        "    plt.title('SAR Image')\n",
        "    ax1.imshow((np.squeeze(sar)[:,:,0]+1)/2., cmap='gray')\n",
        "    ax1.axis('off')\n",
        "\n",
        "    ax2 = fig.add_subplot(132)\n",
        "    plt.title('Target')\n",
        "    ax2.imshow((np.squeeze(real_opt)[:,:,:3]+1)/2.)\n",
        "    ax2.axis('off')\n",
        "\n",
        "    ax2 = fig.add_subplot(133)\n",
        "    plt.title('Predicted')\n",
        "    ax2.imshow((np.squeeze(fake_opt)[:,:,:3]+1)/2.)\n",
        "    ax2.axis('off')\n",
        "\n",
        "    plt.show()\n",
        "    # fig.savefig(save_img_path+name+'_img_pt_br_2_elastic_'+str(epoch))\n",
        "    plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gRpMCwacBZsN"
      },
      "outputs": [],
      "source": [
        "get_next_trn_batch = get_next_batch_generator(patches_idx_trn, S1_img, S2_img, flip=True)\n",
        "get_next_val_batch = get_next_batch_generator(patches_idx_val, S1_img, S2_img, train=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CEtp6_qcBbwp"
      },
      "outputs": [],
      "source": [
        "with tf.Session() as sess:\n",
        "    sess.run(init_op)\n",
        "\n",
        "    if restore:\n",
        "        # Restore variables from disk.\n",
        "        saver.restore(sess, saving_path+name +\".ckpt\")\n",
        "        print(\"Model restored.\")\n",
        "\n",
        "\n",
        "    best_val_loss = np.inf\n",
        "    for epoch in range(epochs):\n",
        "        loss_D, loss_G, loss_l1 = [], [], []\n",
        "\n",
        "        start_time = time.time()\n",
        "        for idx in range(0, num_of_trn_batches):\n",
        "\n",
        "            # selecting a batch of images\n",
        "            batch_sar, batch_opt = next(get_next_trn_batch)\n",
        "\n",
        "            feed_dict={REAL_SAR: batch_sar, REAL_OPT: batch_opt}\n",
        "\n",
        "            # Update D networks\n",
        "            sess.run([d_optim], feed_dict=feed_dict)\n",
        "\n",
        "            # Update G network\n",
        "            for g_ in range(g_steps):\n",
        "                sess.run([g_optim], feed_dict=feed_dict)\n",
        "\n",
        "            with sess.as_default():\n",
        "                errD = d_loss.eval(feed_dict)\n",
        "                errG = g_loss_.eval(feed_dict)\n",
        "                errl1 = reco_loss.eval(feed_dict)\n",
        "                loss_D.append(errD)\n",
        "                loss_G.append(errG)\n",
        "                loss_l1.append(errl1)\n",
        "\n",
        "            if idx % (num_of_trn_batches//1) == 0:\n",
        "                print('Random training samples')\n",
        "                pred = Generator.predict(batch_sar)\n",
        "                plot_images(batch_sar, batch_opt, pred)\n",
        "\n",
        "        # Evaluating model on validation,\n",
        "        val_loss = []\n",
        "        for _ in range(0, num_of_val_batches):\n",
        "            batch_sar, batch_opt = next(get_next_val_batch)\n",
        "            feed_dict={REAL_SAR: batch_sar, REAL_OPT: batch_opt}\n",
        "            val_loss.append(reco_loss.eval(feed_dict))\n",
        "\n",
        "        if best_val_loss > np.mean(val_loss):\n",
        "            patience = 15\n",
        "            best_val_loss = np.mean(val_loss)\n",
        "            print('Saving best model and checkpoints')\n",
        "            save_model(Generator, saving_path+name+'_gen_net.h5')\n",
        "            save_model(Discriminator, saving_path+name+'_dis_net.h5')\n",
        "            # Save the variables to disk.\n",
        "            saver.save(sess, saving_path+name +\".ckpt\")\n",
        "            print('Ok')\n",
        "        else:\n",
        "            patience -= 1\n",
        "        if patience < 0:\n",
        "            break\n",
        "\n",
        "        elapsed_time = time.time() - start_time\n",
        "\n",
        "        print('Epoch: ', epoch, 'Elapsed time:', elapsed_time,  'Val_loss: ', np.mean(val_loss))\n",
        "        print('Dx_loss :', np.mean(loss_D), 'G_loss :', np.mean(loss_G), 'l1_loss :', np.mean(loss_l1))\n",
        "\n",
        "        print('Random validations samples')\n",
        "        pred = Generator.predict(batch_sar)\n",
        "\n",
        "        plot_images(batch_sar, batch_opt, pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YdyGDWhWBe2C"
      },
      "outputs": [],
      "source": [
        "## Cleaning the session to load best model\n",
        "K.clear_session()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D0E7pH4aBi0Y"
      },
      "outputs": [],
      "source": [
        "class sar2opt_model(object):\n",
        "    def __init__(self, model_weights, input_dim=c_dim_sar):\n",
        "        self.model_weights=model_weights\n",
        "        self.input_dim=input_dim\n",
        "        self.gen_net = self.load_gen_model()\n",
        "\n",
        "    def load_gen_model(self):\n",
        "        gen_net = load_model(self.model_weights)\n",
        "        gen_net.layers.pop(0)\n",
        "        new_input = Input(shape=(None, None, self.input_dim), name='input')\n",
        "        new_output = gen_net(new_input)\n",
        "        net = Model(inputs=[new_input], outputs=[new_output], name='sar2optical')\n",
        "        return net\n",
        "\n",
        "    def predict(self, image):\n",
        "        rows, cols = image.shape[:2]\n",
        "        new_img = self.toNewImgSize(image)\n",
        "        pred = self.gen_net.predict(new_img)\n",
        "        pred = np.squeeze(pred)[:rows, :cols]\n",
        "        return pred\n",
        "\n",
        "    def toNewImgSize(self, image):\n",
        "        rows, cols = image.shape[:2]\n",
        "        new_rows = rows + 4 - rows % 4\n",
        "        new_cols = cols + 4 - cols % 4\n",
        "        NewImg = image.max() * np.ones((new_rows, new_cols, c_dim_sar))\n",
        "        NewImg[:rows, :cols] = image.copy()\n",
        "        return np.expand_dims(NewImg,0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UhUEZHqABl6A"
      },
      "outputs": [],
      "source": [
        "gen_model = sar2opt_model('sar2opt_gen_net.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a8EPtXHNBoR5"
      },
      "outputs": [],
      "source": [
        "for tile in tst_tiles:\n",
        "    S1_tst_tile = S1_img.reshape(-1, c_dim_sar)[tiles_grid_idx[tile]]\n",
        "    S2_tst_tile = S2_img.reshape(-1, c_dim_opt)[tiles_grid_idx[tile]]\n",
        "    pred_tst_tile = gen_model.predict(S1_tst_tile)\n",
        "    plot_images(S1_tst_tile, S2_tst_tile, pred_tst_tile, figsize=(20,10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GIYoiCtoBqXR"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}